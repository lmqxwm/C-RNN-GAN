{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5921,
     "status": "ok",
     "timestamp": 1643089914876,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "R5WwrmBakTrm",
    "outputId": "948610ec-2434-42a6-bc43-8cee08220714"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8814,
     "status": "ok",
     "timestamp": 1643096207406,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "h7BW4PrnKkDh",
    "outputId": "9a818807-980f-4d4c-9e6e-331125f55bfc"
   },
   "outputs": [],
   "source": [
    "!pip install tf-slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10646,
     "status": "ok",
     "timestamp": 1643131275219,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "dqIg7qUfm3LT"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time, datetime, os, sys\n",
    "import pickle as pkl\n",
    "from subprocess import call, Popen\n",
    "import tf_slim\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.python.client import timeline\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#os.chdir(\"E:/InterpoMAE/c-rnn-gan-master\")\n",
    "import data_utils\n",
    "import csv\n",
    "# from arg_utils import *\n",
    "\n",
    "model_layout_flags = ['num_layers_g', 'num_layers_d', 'meta_layer_size', 'hidden_size_g', 'hidden_size_d', 'biscale_slow_layer_ticks', 'multiscale', 'multiscale', 'disable_feed_previous', 'pace_events', 'minibatch_d', 'unidirectional_d', 'feature_matching', 'composer']\n",
    "\n",
    "\n",
    "flags = tf.compat.v1.flags\n",
    "logging = tf.compat.v1.logging\n",
    "\n",
    "flags.DEFINE_string(\"datadir\", 'sample_data', \"Directory to save and load midi music files.\")\n",
    "flags.DEFINE_string(\"traindir\", 'stock_output', \"Directory to save checkpoints and gnuplot files.\")\n",
    "flags.DEFINE_integer(\"epochs_per_checkpoint\", 2,\n",
    "                     \"How many training epochs to do per checkpoint.\")\n",
    "flags.DEFINE_boolean(\"log_device_placement\", False,           #\n",
    "                   \"Outputs info on device placement.\")\n",
    "flags.DEFINE_string(\"call_after\", None, \"Call this command after exit.\")\n",
    "flags.DEFINE_integer(\"exit_after\", 1440000,\n",
    "                     \"exit after this many minutes\")\n",
    "flags.DEFINE_integer(\"select_validation_percentage\", None,\n",
    "                     \"Select random percentage of data as validation set.\")\n",
    "flags.DEFINE_integer(\"select_test_percentage\", None,\n",
    "                     \"Select random percentage of data as test set.\")\n",
    "flags.DEFINE_boolean(\"sample\", False,\n",
    "                     \"Sample output from the model. Assume training was already done. Save sample output to file.\")\n",
    "flags.DEFINE_integer(\"works_per_composer\", None,\n",
    "                     \"Limit number of works per composer that is loaded.\")\n",
    "flags.DEFINE_boolean(\"disable_feed_previous\", False,\n",
    "                     \"Feed output from previous cell to the input of the next. In the generator.\")\n",
    "flags.DEFINE_float(\"init_scale\", 0.05,                # .1, .04\n",
    "                   \"the initial scale of the weights\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.1,              # .05,.1,.9 \n",
    "                   \"Learning rate\")\n",
    "flags.DEFINE_float(\"d_lr_factor\", 0.5,                # .5\n",
    "                   \"Learning rate decay\")\n",
    "flags.DEFINE_float(\"max_grad_norm\", 5.0,              # 5.0, 10.0\n",
    "                   \"the maximum permissible norm of the gradient\")\n",
    "flags.DEFINE_float(\"keep_prob\", 0.5,                  # 1.0, .35\n",
    "                   \"Keep probability. 1.0 disables dropout.\")\n",
    "flags.DEFINE_float(\"lr_decay\", 1.0,                   # 1.0\n",
    "                   \"Learning rate decay after each epoch after epochs_before_decay\")\n",
    "flags.DEFINE_integer(\"num_layers_g\", 2,                 # 2\n",
    "                   \"Number of stacked recurrent cells in G.\")\n",
    "flags.DEFINE_integer(\"num_layers_d\", 2,                 # 2\n",
    "                   \"Number of stacked recurrent cells in D.\")\n",
    "flags.DEFINE_integer(\"seq_len\", 24,               # 200, 500\n",
    "                   \"Limit song inputs to this number of events.\")\n",
    "flags.DEFINE_integer(\"meta_layer_size\", 100,          # 300, 600\n",
    "                   \"Size of hidden layer for meta information module.\")\n",
    "flags.DEFINE_integer(\"hidden_size_g\", 75,              # 200, 1500\n",
    "                   \"Hidden size for recurrent part of G.\")###\n",
    "flags.DEFINE_integer(\"hidden_size_d\", 75,              # 200, 1500\n",
    "                   \"Hidden size for recurrent part of D. Default: same as for G.\")###\n",
    "flags.DEFINE_integer(\"epochs_before_decay\", 60,       # 40, 140\n",
    "                   \"Number of epochs before starting to decay.\")\n",
    "flags.DEFINE_integer(\"max_epoch\", 10,                # 500, 500\n",
    "                   \"Number of epochs before stopping training.\")\n",
    "flags.DEFINE_integer(\"batch_size\", 128,                # 10, 20\n",
    "                   \"Batch size.\") ###\n",
    "flags.DEFINE_integer(\"biscale_slow_layer_ticks\", 8,   # 8\n",
    "                   \"Biscale slow layer ticks. Not implemented yet.\")\n",
    "flags.DEFINE_boolean(\"multiscale\", False,             #\n",
    "                   \"Multiscale RNN. Not implemented.\")\n",
    "flags.DEFINE_integer(\"pretraining_epochs\", 6,        # 20, 40\n",
    "                   \"Number of epochs to run lang-model style pretraining.\")\n",
    "flags.DEFINE_boolean(\"pretraining_d\", False,          #\n",
    "                   \"Train D during pretraining.\")\n",
    "flags.DEFINE_boolean(\"initialize_d\", False,           #\n",
    "                   \"Initialize variables for D, no matter if there are trained versions in checkpoint.\")\n",
    "flags.DEFINE_boolean(\"ignore_saved_args\", False,      #\n",
    "                   \"Tells the program to ignore saved arguments, and instead use the ones provided as CLI arguments.\")\n",
    "flags.DEFINE_boolean(\"pace_events\", False,            #\n",
    "                   \"When parsing input data, insert one dummy event at each quarter note if there is no tone.\")\n",
    "flags.DEFINE_boolean(\"minibatch_d\", False,            #\n",
    "                   \"Adding kernel features for minibatch diversity.\")\n",
    "flags.DEFINE_boolean(\"unidirectional_d\", False,        #\n",
    "                   \"Unidirectional RNN instead of bidirectional RNN for D.\")\n",
    "flags.DEFINE_boolean(\"profiling\", False,              #\n",
    "                   \"Profiling. Writing a timeline.json file in plots dir.\")\n",
    "flags.DEFINE_boolean(\"float16\", False,                #\n",
    "                   \"Use floa16 data type. Otherwise, use float32.\")\n",
    "\n",
    "flags.DEFINE_boolean(\"adam\", False,                   #\n",
    "                   \"Use Adam optimizer.\")\n",
    "flags.DEFINE_boolean(\"feature_matching\", False,       #\n",
    "                   \"Feature matching objective for G.\")\n",
    "flags.DEFINE_boolean(\"disable_l2_regularizer\", False,       #\n",
    "                   \"L2 regularization on weights.\")\n",
    "flags.DEFINE_float(\"reg_scale\", 1.0,       #\n",
    "                   \"L2 regularization scale.\")\n",
    "flags.DEFINE_boolean(\"synthetic_chords\", False,       #\n",
    "                   \"Train on synthetically generated chords (three tones per event).\")\n",
    "flags.DEFINE_integer(\"tones_per_cell\", 1,             # 2,3\n",
    "                   \"Maximum number of tones to output per RNN cell.\")\n",
    "flags.DEFINE_string(\"composer\", None, \"Specify exactly one composer, and train model only on this.\")\n",
    "flags.DEFINE_float(\"random_input_scale\", 1.0,       #\n",
    "                   \"Scale of random inputs (1.0=same size as generated features).\")\n",
    "flags.DEFINE_boolean(\"end_classification\", False, \"Classify only in ends of D. Otherwise, does classification at every timestep and mean reduce.\")\n",
    "flags.DEFINE_string(\"f\", \"\", \"kernel\") \n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3083,
     "status": "ok",
     "timestamp": 1643131278300,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "q2FL83p1nGf-"
   },
   "outputs": [],
   "source": [
    "def make_rnn_cell(rnn_layer_sizes,\n",
    "                  dropout_keep_prob=1.0,\n",
    "                  attn_length=0,\n",
    "                  base_cell=tf.compat.v1.nn.rnn_cell.LSTMCell,\n",
    "                  state_is_tuple=True,\n",
    "                  reuse=False):\n",
    "  \"\"\"Makes a RNN cell from the given hyperparameters.\n",
    "\n",
    "  Args:\n",
    "    rnn_layer_sizes: A list of integer sizes (in units) for each layer of the RNN.\n",
    "    dropout_keep_prob: The float probability to keep the output of any given sub-cell.\n",
    "    attn_length: The size of the attention vector.\n",
    "    base_cell: The base tf.contrib.rnn.RNNCell to use for sub-cells.\n",
    "    state_is_tuple: A boolean specifying whether to use tuple of hidden matrix\n",
    "        and cell matrix as a state instead of a concatenated matrix.\n",
    "\n",
    "  Returns:\n",
    "      A tf.contrib.rnn.MultiRNNCell based on the given hyperparameters.\n",
    "  \"\"\"\n",
    "  cells = []\n",
    "  for num_units in rnn_layer_sizes:\n",
    "    cell = base_cell(num_units, state_is_tuple=state_is_tuple, reuse=reuse)\n",
    "    cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(\n",
    "        cell, output_keep_prob=dropout_keep_prob)\n",
    "    cells.append(cell)\n",
    "\n",
    "  cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=state_is_tuple)\n",
    "  if attn_length:\n",
    "    cell = tfa.seq2seq.AttentionWrapper(\n",
    "        cell, attn_length, state_is_tuple=state_is_tuple, reuse=reuse)\n",
    "\n",
    "  return cell\n",
    "\n",
    "def restore_flags(save_if_none_found=True):\n",
    "  if FLAGS.traindir:\n",
    "    saved_args_dir = os.path.join(FLAGS.traindir, 'saved_args')\n",
    "    if save_if_none_found:\n",
    "      try: os.makedirs(saved_args_dir)\n",
    "      except: pass\n",
    "    for arg in FLAGS.__flags:\n",
    "      if arg not in model_layout_flags:\n",
    "        continue\n",
    "      if FLAGS.ignore_saved_args and os.path.exists(os.path.join(saved_args_dir, arg+'.pkl')):\n",
    "        print('{:%Y-%m-%d %H:%M:%S}: saved_args: Found {} setting from saved state, but using CLI args ({}) and saving (--ignore_saved_args).'.format(datetime.datetime.today(), arg, getattr(FLAGS, arg)))\n",
    "      elif os.path.exists(os.path.join(saved_args_dir, arg+'.pkl')):\n",
    "        with open(os.path.join(saved_args_dir, arg+'.pkl'), 'rb') as f:\n",
    "          setattr(FLAGS, arg, pkl.load(f))\n",
    "          print('{:%Y-%m-%d %H:%M:%S}: saved_args: {} from saved state ({}), ignoring CLI args.'.format(datetime.datetime.today(), arg, getattr(FLAGS, arg)))\n",
    "      elif save_if_none_found:\n",
    "        print('{:%Y-%m-%d %H:%M:%S}: saved_args: Found no {} setting from saved state, using CLI args ({}) and saving.'.format(datetime.datetime.today(), arg, getattr(FLAGS, arg)))\n",
    "        with open(os.path.join(saved_args_dir, arg+'.pkl'), 'wb') as f:\n",
    "            print(getattr(FLAGS, arg),arg)\n",
    "            pkl.dump(getattr(FLAGS, arg), f)\n",
    "      else:\n",
    "        print('{:%Y-%m-%d %H:%M:%S}: saved_args: Found no {} setting from saved state, using CLI args ({}) but not saving.'.format(datetime.datetime.today(), arg, getattr(FLAGS, arg)))\n",
    "\n",
    "def data_type():\n",
    "  return tf.float16 if FLAGS.float16 else tf.float32\n",
    "  #return tf.float16\n",
    "\n",
    "def my_reduce_mean(what_to_take_mean_over):\n",
    "  return tf.reshape(what_to_take_mean_over, shape=[-1])[0]\n",
    "  denom = 1.0\n",
    "  #print(what_to_take_mean_over.get_shape())\n",
    "  for d in what_to_take_mean_over.get_shape():\n",
    "    #print(d)\n",
    "    if type(d) == tf.Dimension:\n",
    "      denom = denom*d.value\n",
    "    else:\n",
    "      denom = denom*d\n",
    "  return tf.reduce_sum(what_to_take_mean_over)/denom\n",
    "\n",
    "def linear(inp, output_dim, scope=None, stddev=1.0, reuse_scope=False):\n",
    "  norm = tf.compat.v1.random_normal_initializer(stddev=stddev, dtype=data_type())\n",
    "  const = tf.constant_initializer(0.0)\n",
    "  with tf.compat.v1.variable_scope(scope or 'linear') as scope:\n",
    "    scope.set_regularizer(tf.keras.regularizers.l2(FLAGS.reg_scale))\n",
    "    if reuse_scope:\n",
    "      scope.reuse_variables()\n",
    "    #print('inp.get_shape(): {}'.format(inp.get_shape()))\n",
    "    w = tf.compat.v1.get_variable('w', [inp.get_shape()[1], output_dim], initializer=norm, dtype=data_type())\n",
    "    b = tf.compat.v1.get_variable('b', [output_dim], initializer=const, dtype=data_type())\n",
    "  return tf.matmul(inp, w) + b\n",
    "\n",
    "def minibatch(inp, num_kernels=25, kernel_dim=10, scope=None, msg='', reuse_scope=False):\n",
    "  \"\"\"\n",
    "   Borrowed from http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/\n",
    "  \"\"\"\n",
    "  with tf.compat.v1.variable_scope(scope or 'minibatch_d') as scope:\n",
    "    scope.set_regularizer(tf.keras.regularizers.l2(FLAGS.reg_scale))\n",
    "    if reuse_scope:\n",
    "      scope.reuse_variables()\n",
    "  \n",
    "    inp = tf.compat.v1.Print(inp, [inp],\n",
    "            '{} inp = '.format(msg), summarize=20, first_n=20)\n",
    "    x = tf.sigmoid(linear(inp, num_kernels * kernel_dim, scope))\n",
    "    activation = tf.reshape(x, (-1, num_kernels, kernel_dim))\n",
    "    activation = tf.compat.v1.Print(activation, [activation],\n",
    "            '{} activation = '.format(msg), summarize=20, first_n=20)\n",
    "    diffs = tf.expand_dims(activation, 3) - \\\n",
    "                tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)\n",
    "    diffs = tf.compat.v1.Print(diffs, [diffs],\n",
    "            '{} diffs = '.format(msg), summarize=20, first_n=20)\n",
    "    abs_diffs = tf.reduce_sum(tf.abs(diffs), 2)\n",
    "    abs_diffs = tf.compat.v1.Print(abs_diffs, [abs_diffs],\n",
    "            '{} abs_diffs = '.format(msg), summarize=20, first_n=20)\n",
    "    minibatch_features = tf.reduce_sum(tf.exp(-abs_diffs), 2)\n",
    "    minibatch_features = tf.compat.v1.Print(minibatch_features, [tf.reduce_min(minibatch_features), tf.reduce_max(minibatch_features)],\n",
    "            '{} minibatch_features (min,max) = '.format(msg), summarize=20, first_n=20)\n",
    "  return tf.concat( [inp, minibatch_features],1)\n",
    "\n",
    "class RNNGAN(object):\n",
    "  \"\"\"The RNNGAN model.\"\"\"\n",
    "\n",
    "  def __init__(self, is_training, num_features):\n",
    "    batch_size = FLAGS.batch_size\n",
    "    self.batch_size =  batch_size\n",
    "\t\n",
    "    seq_len = FLAGS.seq_len\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "    print('sequence length: {}'.format(self.seq_len))\n",
    "    print('batch size: {}'.format(batch_size))\n",
    "    print('number of features: {}'.format(num_features))\n",
    "    self._input_data = tf.compat.v1.placeholder(shape=[batch_size, seq_len, num_features], dtype=data_type())\n",
    "    data_inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(self._input_data, seq_len, 1)]\n",
    "    #data_inputs = self._input_data\n",
    "    \n",
    "    with tf.compat.v1.variable_scope('G', reuse=tf.compat.v1.AUTO_REUSE) as scope:\n",
    "      scope.set_regularizer(tf.keras.regularizers.l2(FLAGS.reg_scale))\n",
    "      #lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(FLAGS.hidden_size_g, forget_bias=1.0, state_is_tuple=True)\n",
    "      if is_training and FLAGS.keep_prob < 1:\n",
    "        #lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=FLAGS.keep_prob)\n",
    "        cell = make_rnn_cell([FLAGS.hidden_size_g]*FLAGS.num_layers_g, dropout_keep_prob=FLAGS.keep_prob)\n",
    "      else:\n",
    "        cell = make_rnn_cell([FLAGS.hidden_size_g]*FLAGS.num_layers_g)\t  \n",
    "\n",
    "      #cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell for _ in range( FLAGS.num_layers_g)], state_is_tuple=True)\n",
    "      self._initial_state = cell.zero_state(batch_size, data_type())\n",
    "\n",
    "\n",
    "      random_rnninputs = tf.random.uniform(shape=[batch_size, seq_len, int(FLAGS.random_input_scale*num_features)], minval=0.0, maxval=1.0, dtype=data_type())\n",
    "\n",
    "      # Make list of tensors. One per step in recurrence.\n",
    "      # Each tensor is batchsize*numfeatures.\n",
    "      \n",
    "      random_rnninputs = [tf.squeeze(input_, [1]) for input_ in tf.split(random_rnninputs, seq_len, 1)]\n",
    "      \n",
    "      # REAL GENERATOR:\n",
    "      state = self._initial_state\n",
    "      # as we feed the output as the input to the next, we 'invent' the initial 'output'.\n",
    "      generated_point = tf.random.uniform(shape=[batch_size, num_features], minval=0.0, maxval=1.0, dtype=data_type())\n",
    "      outputs = []\n",
    "      self._generated_features = []\n",
    "      for i,input_ in enumerate(random_rnninputs):\n",
    "        if i > 0: scope.reuse_variables()\n",
    "        concat_values = [input_]\n",
    "        if not FLAGS.disable_feed_previous:\n",
    "          concat_values.append(generated_point)\n",
    "        if len(concat_values):\n",
    "          input_ = tf.concat(axis=1, values=concat_values)\n",
    "        input_ = tf.nn.relu(linear(input_, FLAGS.hidden_size_g, scope='input_layer', reuse_scope=(i!=0)))\n",
    "        output, state = cell(input_, state)\n",
    "        outputs.append(output)\n",
    "        #generated_point = tf.nn.relu(linear(output, num_song_features, scope='output_layer', reuse_scope=(i!=0)))\n",
    "        generated_point = linear(output, num_features, scope='output_layer', reuse_scope=(i!=0))\n",
    "        self._generated_features.append(generated_point)\n",
    "      \n",
    "      \n",
    "      # PRETRAINING GENERATOR, will feed inputs, not generated outputs:\n",
    "      scope.reuse_variables()\n",
    "      # as we feed the output as the input to the next, we 'invent' the initial 'output'.\n",
    "      prev_target = tf.random.uniform(shape=[batch_size, num_features], minval=0.0, maxval=1.0, dtype=data_type())\n",
    "      outputs = []\n",
    "      self._generated_features_pretraining = []\n",
    "      for i,input_ in enumerate(random_rnninputs):\n",
    "        concat_values = [input_]\n",
    "        if not FLAGS.disable_feed_previous:\n",
    "          concat_values.append(prev_target)\n",
    "        if len(concat_values):\n",
    "          input_ = tf.concat(axis=1, values=concat_values)\n",
    "        input_ = tf.nn.relu(linear(input_, FLAGS.hidden_size_g, scope='input_layer', reuse_scope=(i!=0)))\n",
    "        output, state = cell(input_, state)\n",
    "        outputs.append(output)\n",
    "        #generated_point = tf.nn.relu(linear(output, num_song_features, scope='output_layer', reuse_scope=(i!=0)))\n",
    "        generated_point = linear(output, num_features, scope='output_layer', reuse_scope=(i!=0))\n",
    "        self._generated_features_pretraining.append(generated_point)\n",
    "        prev_target = data_inputs[i]\n",
    "      \n",
    "      #outputs, state = tf.nn.rnn(cell, transformed, initial_state=self._initial_state)\n",
    "\n",
    "      #self._generated_features = [tf.nn.relu(linear(output, num_song_features, scope='output_layer', reuse_scope=(i!=0))) for i,output in enumerate(outputs)]\n",
    "\n",
    "    self._final_state = state\n",
    "\n",
    "    # These are used both for pretraining and for D/G training further down.\n",
    "    self._lr = tf.Variable(FLAGS.learning_rate, trainable=False, dtype=data_type())\n",
    "    self.g_params = [v for v in tf.compat.v1.trainable_variables() if v.name.startswith('model/G/')]\n",
    "    if FLAGS.adam:\n",
    "      g_optimizer = tf.train.AdamOptimizer(self._lr)\n",
    "    else:\n",
    "      g_optimizer = tf.compat.v1.train.GradientDescentOptimizer(self._lr)\n",
    "   \n",
    "    reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    reg_constant = 0.1  # Choose an appropriate one.\n",
    "    reg_loss = reg_constant * sum(reg_losses)\n",
    "    reg_loss = tf.compat.v1.Print(reg_loss, reg_losses, 'reg_losses = ', summarize=20, first_n=20)\n",
    "    #if not FLAGS.disable_l2_regularizer:\n",
    "    #  print('L2 regularization. Reg losses: {}'.format([v.name for v in reg_losses]))\n",
    "   \n",
    "    print(\"---BEGIN, PRETRAINING. ---\")\n",
    "    \n",
    "    print(tf.transpose(tf.stack(self._generated_features_pretraining), perm=[1, 0, 2]).get_shape())\n",
    "    print(self._input_data.get_shape())\n",
    "    self.rnn_pretraining_loss = tf.compat.v1.reduce_mean(tf.math.squared_difference(x=tf.transpose(tf.stack(self._generated_features_pretraining), perm=[1, 0, 2]), y=self._input_data)) \\\n",
    "                 # + tf.compat.v1.reduce_mean(tf.compat.v1.losses.absolute_difference(tf.transpose(tf.stack(self._generated_features_pretraining), perm=[1, 0, 2]), self._input_data))\n",
    "                 #############pretraining loss\n",
    "    if not FLAGS.disable_l2_regularizer:\n",
    "      self.rnn_pretraining_loss = self.rnn_pretraining_loss+reg_loss\n",
    "    \n",
    "    \n",
    "    pretraining_grads, _ = tf.clip_by_global_norm(tf.gradients(self.rnn_pretraining_loss, self.g_params), FLAGS.max_grad_norm)\n",
    "    self.opt_pretraining = g_optimizer.apply_gradients(zip(pretraining_grads, self.g_params))\n",
    "\n",
    "    print(\"---END, PRETRAINING---\")\n",
    "\n",
    "    # The discriminator tries to tell the difference between samples from the\n",
    "    # true data distribution (self.x) and the generated samples (self.z).\n",
    "    #\n",
    "    # Here we create two copies of the discriminator network (that share parameters),\n",
    "    # as you cannot use the same network with different inputs in TensorFlow.\n",
    "    with tf.compat.v1.variable_scope('D', reuse=tf.compat.v1.AUTO_REUSE) as scope:\n",
    "      scope.set_regularizer(tf.keras.regularizers.l2(FLAGS.reg_scale))\n",
    "      # Make list of tensors. One per step in recurrence.\n",
    "      # Each tensor is batchsize*numfeatures.\n",
    "      # TODO: (possibly temporarily) disabling meta info\n",
    "      print('self._input_data shape {}'.format(self._input_data.get_shape()))\n",
    "      print('generated data shape {}'.format(self._generated_features[0].get_shape()))\n",
    "      # TODO: (possibly temporarily) disabling meta info\n",
    "      self.real_d,self.real_d_features = self.discriminator(data_inputs, is_training, msg='real')\n",
    "      scope.reuse_variables()\n",
    "      # TODO: (possibly temporarily) disabling meta info\n",
    "      generated_data = self._generated_features\n",
    "      if data_inputs[0].get_shape() != generated_data[0].get_shape():\n",
    "        print('data_inputs shape {} != generated data shape {}'.format(data_inputs[0].get_shape(), generated_data[0].get_shape()))\n",
    "      self.generated_d,self.generated_d_features = self.discriminator(generated_data, is_training, msg='generated')\n",
    "\n",
    "    # Define the loss for discriminator and generator networks (see the original\n",
    "    # paper for details), and create optimizers for both\n",
    "    # self.d_loss = tf.compat.v1.reduce_mean(-tf.math.log(tf.clip_by_value(self.real_d, 1e-1000000, 1.0)) \\\n",
    "    #                             -tf.math.log(1 - tf.clip_by_value(self.generated_d, 0.0, 1.0-1e-1000000)))\n",
    "    # self.g_loss_feature_matching = tf.reduce_sum(tf.math.squared_difference(self.real_d_features, self.generated_d_features))\n",
    "    # self.g_loss = tf.compat.v1.reduce_mean(tf.math.squared_difference(x=tf.transpose(tf.stack(self._generated_features), perm=[1, 0, 2]), y=self._input_data)) \\\n",
    "    #         + tf.compat.v1.reduce_mean(tf.compat.v1.losses.absolute_difference(tf.transpose(tf.stack(self._generated_features), perm=[1, 0, 2]), self._input_data))  \\\n",
    "    #         + tf.compat.v1.reduce_mean(-tf.math.log(tf.clip_by_value(self.generated_d, 1e-1000000, 1.0)))\n",
    "    self.g_loss = tf.compat.v1.reduce_mean(-tf.math.log(tf.clip_by_value(self.generated_d, 1e-1000000, 1.0)))      \n",
    "    \n",
    "    # self.g_loss = tf.compat.v1.convert_to_tensor(0.0)\n",
    "    self.g_loss_feature_matching = tf.compat.v1.reduce_mean(tf.math.abs(tf.math.sqrt(tf.compat.v1.nn.moments(self.real_d_features, axes=[1])[1]+1e-6)-tf.math.sqrt(tf.compat.v1.nn.moments(self.generated_d_features, axes=[1])[1]+1e-6))) + tf.compat.v1.reduce_mean(tf.math.abs(tf.math.sqrt(tf.compat.v1.nn.moments(self.real_d_features, axes=[1])[0]+1e-6)-tf.math.sqrt(tf.compat.v1.nn.moments(self.generated_d_features, axes=[1])[0]+1e-6)))\n",
    "    self.d_loss = tf.compat.v1.reduce_mean(tf.math.abs(tf.math.sqrt(tf.compat.v1.nn.moments(self.real_d, axes=[0])[1]+1e-6)-tf.math.sqrt(tf.compat.v1.nn.moments(self.generated_d, axes=[0])[1]+1e-6))) + tf.compat.v1.reduce_mean(tf.math.abs(tf.math.sqrt(tf.compat.v1.nn.moments(self.real_d, axes=[0])[0]+1e-6)-tf.math.sqrt(tf.compat.v1.nn.moments(self.generated_d, axes=[0])[0]+1e-6)))\n",
    "\n",
    "\n",
    "    if not FLAGS.disable_l2_regularizer:\n",
    "      self.d_loss = self.d_loss+reg_loss\n",
    "      self.g_loss_feature_matching = self.g_loss_feature_matching+reg_loss\n",
    "      self.g_loss = self.g_loss+reg_loss\n",
    "    self.d_params = [v for v in tf.compat.v1.trainable_variables() if v.name.startswith('model/D/')]\n",
    "\n",
    "    if not is_training:\n",
    "      return\n",
    "\n",
    "    d_optimizer = tf.compat.v1.train.GradientDescentOptimizer(self._lr*FLAGS.d_lr_factor)\n",
    "    d_grads, _ = tf.clip_by_global_norm(tf.gradients(self.d_loss, self.d_params), FLAGS.max_grad_norm)\n",
    "    self.opt_d = d_optimizer.apply_gradients(zip(d_grads, self.d_params))\n",
    "    if FLAGS.feature_matching:\n",
    "      g_grads, _ = tf.clip_by_global_norm(tf.gradients(self.g_loss_feature_matching,\n",
    "                                                       self.g_params),\n",
    "                                        FLAGS.max_grad_norm)\n",
    "    else:\n",
    "      g_grads, _ = tf.clip_by_global_norm(tf.gradients(self.g_loss, self.g_params),\n",
    "                                        FLAGS.max_grad_norm)\n",
    "    self.opt_g = g_optimizer.apply_gradients(zip(g_grads, self.g_params))\n",
    "\n",
    "    self._new_lr = tf.compat.v1.placeholder(shape=[], name=\"new_learning_rate\", dtype=data_type())\n",
    "    self._lr_update = tf.compat.v1.assign(self._lr, self._new_lr)\n",
    "\n",
    "  def discriminator(self, inputs, is_training, msg=''):\n",
    "    # RNN discriminator:\n",
    "    #for i in xrange(len(inputs)):\n",
    "    #  print('shape inputs[{}] {}'.format(i, inputs[i].get_shape()))\n",
    "    #inputs[0] = tf.Print(inputs[0], [inputs[0]],\n",
    "    #        '{} inputs[0] = '.format(msg), summarize=20, first_n=20)\n",
    "    if is_training and FLAGS.keep_prob < 1:\n",
    "      inputs = [tf.nn.dropout(input_, FLAGS.keep_prob) for input_ in inputs]\n",
    "    \n",
    "    #lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(FLAGS.hidden_size_d, forget_bias=1.0, state_is_tuple=True)\n",
    "    if is_training and FLAGS.keep_prob < 1:\n",
    "      #lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "      #lstm_cell, output_keep_prob=FLAGS.keep_prob)\n",
    "      cell_fw = make_rnn_cell([FLAGS.hidden_size_d]* FLAGS.num_layers_d, dropout_keep_prob=FLAGS.keep_prob)\n",
    "      \n",
    "      cell_bw = make_rnn_cell([FLAGS.hidden_size_d]* FLAGS.num_layers_d, dropout_keep_prob=FLAGS.keep_prob)\n",
    "    else:\n",
    "      cell_fw = make_rnn_cell([FLAGS.hidden_size_d]* FLAGS.num_layers_d)\n",
    "      \n",
    "      cell_bw = make_rnn_cell([FLAGS.hidden_size_d]* FLAGS.num_layers_d)\n",
    "    #cell_fw = tf.nn.rnn_cell.MultiRNNCell([lstm_cell for _ in range( FLAGS.num_layers_d)], state_is_tuple=True)\n",
    "    self._initial_state_fw = cell_fw.zero_state(self.batch_size, data_type())\n",
    "    if not FLAGS.unidirectional_d:\n",
    "      #lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(FLAGS.hidden_size_g, forget_bias=1.0, state_is_tuple=True)\n",
    "      #if is_training and FLAGS.keep_prob < 1:\n",
    "      #  lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "      #      lstm_cell, output_keep_prob=FLAGS.keep_prob)\n",
    "      #cell_bw = tf.nn.rnn_cell.MultiRNNCell([lstm_cell for _ in range( FLAGS.num_layers_d)], state_is_tuple=True)\n",
    "      self._initial_state_bw = cell_bw.zero_state(self.batch_size, data_type())\n",
    "      print(\"cell_fw\",cell_fw.output_size)\n",
    "      #print(\"cell_bw\",cell_bw.output_size)\n",
    "      #print(\"inputs\",inputs)\n",
    "      #print(\"initial_state_fw\",self._initial_state_fw)\n",
    "      #print(\"initial_state_bw\",self._initial_state_bw)\n",
    "      outputs, state_fw, state_bw = tf.compat.v1.nn.static_bidirectional_rnn(cell_fw, cell_bw, inputs, initial_state_fw=self._initial_state_fw, initial_state_bw=self._initial_state_bw)\n",
    "      #outputs[0] = tf.Print(outputs[0], [outputs[0]],\n",
    "      #        '{} outputs[0] = '.format(msg), summarize=20, first_n=20)\n",
    "      #state = tf.concat(state_fw, state_bw)\n",
    "      #endoutput = tf.concat(concat_dim=1, values=[outputs[0],outputs[-1]])\n",
    "    else:\n",
    "      outputs, state = tf.nn.rnn(cell_fw, inputs, initial_state=self._initial_state_fw)\n",
    "      #state = self._initial_state\n",
    "\t  \n",
    "      #outputs, state = cell_fw(tf.convert_to_tensor (inputs),state)\n",
    "      #endoutput = outputs[-1]\n",
    "\n",
    "    if FLAGS.end_classification:\n",
    "      decisions = [tf.sigmoid(linear(output, 1, 'decision', reuse_scope=(i!=0))) for i,output in enumerate([outputs[0], outputs[-1]])]\n",
    "      decisions = tf.stack(decisions)\n",
    "      decisions = tf.transpose(decisions, perm=[1,0,2])\n",
    "      print('shape, decisions: {}'.format(decisions.get_shape()))\n",
    "    else:\n",
    "      decisions = [tf.sigmoid(linear(output, 1, 'decision', reuse_scope=(i!=0))) for i,output in enumerate(outputs)]\n",
    "      decisions = tf.stack(decisions)\n",
    "      decisions = tf.transpose(decisions, perm=[1,0,2])\n",
    "      print('shape, decisions: {}'.format(decisions.get_shape()))\n",
    "    decision = tf.compat.v1.reduce_mean(decisions, reduction_indices=[1,2])\n",
    "    decision = tf.compat.v1.Print(decision, [decision],\n",
    "            '{} decision = '.format(msg), summarize=20, first_n=20)\n",
    "    return (decision,tf.transpose(tf.stack(outputs), perm=[1,0,2]))\n",
    "      \n",
    "\n",
    "  \n",
    "  def assign_lr(self, session, lr_value):\n",
    "    session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "  @property\n",
    "  def generated_features(self):\n",
    "    return self._generated_features\n",
    "\n",
    "  @property\n",
    "  def input_data(self):\n",
    "    return self._input_data\n",
    "\n",
    "  @property\n",
    "  def targets(self):\n",
    "    return self._targets\n",
    "\n",
    "  @property\n",
    "  def initial_state(self):\n",
    "    return self._initial_state\n",
    "\n",
    "  @property\n",
    "  def cost(self):\n",
    "    return self._cost\n",
    "\n",
    "  @property\n",
    "  def final_state(self):\n",
    "    return self._final_state\n",
    "\n",
    "  @property\n",
    "  def lr(self):\n",
    "    return self._lr\n",
    "\n",
    "  @property\n",
    "  def train_op(self):\n",
    "    return self._train_op\n",
    "\n",
    "def run_epoch(session, model, loader, datasetlabel, eval_op_g, eval_op_d, pretraining=False, verbose=False, pretraining_d=False, predictive=False):\n",
    "  \"\"\"Runs the model on the given data.\"\"\"\n",
    "  #epoch_size = ((len(data) // model.batch_size) - 1) // model.songlength\n",
    "  epoch_start_time = time.time()\n",
    "  g_loss, d_loss = 10.0, 10.0\n",
    "  g_losses, d_losses = 0.0, 0.0\n",
    "  iters = 0\n",
    "  #state = session.run(model.initial_state)\n",
    "  time_before_graph = None\n",
    "  time_after_graph = None\n",
    "  times_in_graph = []\n",
    "  times_in_python = []\n",
    "  #times_in_batchreading = []\n",
    "\n",
    "  print(\"model.batch_size\", model.batch_size)\n",
    "  print(\"datasetlabel\", datasetlabel)\n",
    "  if not predictive:\n",
    "    batch_data = loader.get_batch(model.batch_size, part=datasetlabel)\n",
    "  else:\n",
    "    batch_data = loader.get_seq_batch(model.batch_size, part=datasetlabel)\n",
    "  \n",
    "  run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\n",
    "\n",
    "  while batch_data is not None and iters < 10:\n",
    "    op_g = eval_op_g\n",
    "    op_d = eval_op_d\n",
    "    if datasetlabel == 'train' and not pretraining: # and not FLAGS.feature_matching:\n",
    "      if d_loss == 0.0 and g_loss == 0.0:\n",
    "        print('Both G and D train loss are zero. Exiting.')\n",
    "        break\n",
    "        #saver.save(session, checkpoint_path, global_step=m.global_step)\n",
    "        #break\n",
    "      elif d_loss == 0.0:\n",
    "        #print('D train loss is zero. Freezing optimization. G loss: {:.3f}'.format(g_loss))\n",
    "        op_g = tf.no_op()\n",
    "      elif g_loss == 0.0: \n",
    "        #print('G train loss is zero. Freezing optimization. D loss: {:.3f}'.format(d_loss))\n",
    "        op_d = tf.no_op()\n",
    "      elif g_loss < 2.0 or d_loss < 2.0:\n",
    "        if g_loss*.7 > d_loss:\n",
    "          #print('G train loss is {:.3f}, D train loss is {:.3f}. Freezing optimization of D'.format(g_loss, d_loss))\n",
    "          op_g = tf.no_op()\n",
    "        #elif d_loss*.7 > g_loss:\n",
    "          #print('G train loss is {:.3f}, D train loss is {:.3f}. Freezing optimization of G'.format(g_loss, d_loss))\n",
    "        op_d = tf.no_op()\n",
    "    #fetches = [model.cost, model.final_state, eval_op]\n",
    "    if pretraining:\n",
    "      if pretraining_d:\n",
    "        fetches = [model.rnn_pretraining_loss, model.d_loss, op_g, op_d]\n",
    "      else:\n",
    "        fetches = [model.rnn_pretraining_loss, tf.no_op(), op_g, op_d]\n",
    "    else:\n",
    "      fetches = [model.g_loss, model.d_loss, op_g, op_d]\n",
    "    feed_dict = {}\n",
    "    feed_dict[model.input_data.name] = batch_data\n",
    "    #print(batch_song)\n",
    "    #print(batch_song.shape)\n",
    "    \n",
    "    #for i, (c, h) in enumerate(model.initial_state):\n",
    "    #  feed_dict[c] = state[i].c\n",
    "    #  feed_dict[h] = state[i].h\n",
    "    #cost, state, _ = session.run(fetches, feed_dict)\n",
    "    time_before_graph = time.time()\n",
    "    if iters > 0:\n",
    "      times_in_python.append(time_before_graph-time_after_graph)\n",
    "    g_loss, d_loss, _, _ = session.run(fetches, feed_dict)\n",
    "    time_after_graph = time.time()\n",
    "    if iters > 0:\n",
    "      times_in_graph.append(time_after_graph-time_before_graph)\n",
    "    g_losses += g_loss\n",
    "    if not pretraining:\n",
    "      d_losses += d_loss\n",
    "    iters += 1\n",
    "\n",
    "    if verbose and iters % 10 == 9:\n",
    "      samples_per_sec = float(iters * model.batch_size)/float(time.time() - epoch_start_time)\n",
    "      avg_time_in_graph = float(sum(times_in_graph))/float(len(times_in_graph))\n",
    "      avg_time_in_python = float(sum(times_in_python))/float(len(times_in_python))\n",
    "      #avg_time_batchreading = float(sum(times_in_batchreading))/float(len(times_in_batchreading))\n",
    "      if pretraining:\n",
    "        print(\"{}: {} (pretraining) batch loss: G: {:.3f}, avg loss: G: {:.3f}, speed: {:.1f} samples/s, avg in graph: {:.1f}, avg in python: {:.1f}.\".format(datasetlabel, iters, g_loss, float(g_losses)/float(iters), samples_per_sec, avg_time_in_graph, avg_time_in_python))\n",
    "      else:\n",
    "        print(\"{}: {} batch loss: G: {:.3f}, D: {:.3f}, avg loss: G: {:.3f}, D: {:.3f} speed: {:.1f} samples/s, avg in graph: {:.1f}, avg in python: {:.1f}.\".format(datasetlabel, iters, g_loss, d_loss, float(g_losses)/float(iters), float(d_losses)/float(iters), samples_per_sec, avg_time_in_graph, avg_time_in_python))\n",
    "    #batchtime = time.time()\n",
    "    #batch_data = loader.get_batch(model.batch_size, part=datasetlabel)\n",
    "    #times_in_batchreading.append(time.time()-batchtime)\n",
    "\n",
    "  if iters == 0:\n",
    "    return None\n",
    "\n",
    "  g_mean_loss = g_losses/iters\n",
    "  if pretraining and not pretraining_d:\n",
    "    d_mean_loss = None\n",
    "  else:\n",
    "    d_mean_loss = d_losses/iters\n",
    "  return (g_mean_loss, d_mean_loss)\n",
    "\n",
    "def sample(session, model, batch=False):\n",
    "  \"\"\"Samples from the generative model.\"\"\"\n",
    "  #state = session.run(model.initial_state)\n",
    "  fetches = [model.generated_features]\n",
    "  feed_dict = {}\n",
    "  generated_features, = session.run(fetches, feed_dict)\n",
    "  print(\"generated_features.length\", len(generated_features))\n",
    "  print(\"generated_features[0].shape:\", generated_features[0].shape)\n",
    "  # The following worked when batch_size=1.\n",
    "  # generated_features = [np.squeeze(x, axis=0) for x in generated_features]\n",
    "  # If batch_size != 1, we just pick the first sample. Wastefull, yes.\n",
    "  returnable = []\n",
    "  if batch:\n",
    "    for batchno in range(generated_features[0].shape[0]):\n",
    "      returnable.append([x[batchno,:] for x in generated_features])\n",
    "  else:\n",
    "    returnable = np.array(generated_features).transpose([1,0,2])\n",
    "    #returnable = np.array(tf.transpose(tf.stack(generated_features), perm=[1, 0, 2]))\n",
    "  return returnable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnrZo4ugCyLs"
   },
   "outputs": [],
   "source": [
    "# results = []\n",
    "# with tf.Graph().as_default(), tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=FLAGS.log_device_placement)) as session:\n",
    "#   with tf.compat.v1.variable_scope(\"model\", reuse=True) as scope:\n",
    "#     m = RNNGAN(is_training=False, num_features=num_features)\n",
    "#     all_data = loader.get_all_oridata()\n",
    "#     num_sample = len(all_data)\n",
    "#     iters = int(num_sample/m.batch_size)\n",
    "#     for i in range(iters+1):\n",
    "#       if i < iters:\n",
    "#         batch_data = all_data[i*m.batch_size:(i+1)*m.batch_size] \n",
    "#       else:\n",
    "#         batch_data = all_data[num_sample-m.batch_size:num_sample]\n",
    "#       run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\n",
    "#       op_g = tf.no_op()\n",
    "#       op_d = tf.no_op()\n",
    "#       feed_dict={}\n",
    "#       feed_dict[m.input_data.name] = batch_data\n",
    "#       fetches = [m.g_loss, m.d_loss, op_g, op_d]\n",
    "#       generation = session.run(fetches, feed_dict)\n",
    "#       print(generation)\n",
    "#       if i>1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1643131394203,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "0XzdhbZEzc7U"
   },
   "outputs": [],
   "source": [
    "def adjust_para(hidden_size_g=None, hidden_size_d=None, max_epoch=None, pretraining_epochs=None, reg_scale=None, traindir=None, batch_size=None, seq_len=None):\n",
    "  if hidden_size_g:\n",
    "    del FLAGS.hidden_size_g\n",
    "    flags.DEFINE_integer(\"hidden_size_g\", hidden_size_g, \"1\")\n",
    "  if hidden_size_d:\n",
    "    del FLAGS.hidden_size_d\n",
    "    flags.DEFINE_integer(\"hidden_size_d\", hidden_size_d, \"2\")\n",
    "  if max_epoch:\n",
    "    del FLAGS.max_epoch\n",
    "    flags.DEFINE_integer(\"max_epoch\", max_epoch, \"3\")\n",
    "  if pretraining_epochs:\n",
    "    del FLAGS.pretraining_epochs\n",
    "    flags.DEFINE_integer(\"pretraining_epochs\", pretraining_epochs, \"4\")\n",
    "  if reg_scale:\n",
    "    del FLAGS.reg_scale\n",
    "    flags.DEFINE_float(\"reg_scale\", reg_scale, \"5\")\n",
    "  if traindir:\n",
    "    del FLAGS.traindir\n",
    "    flags.DEFINE_string(\"traindir\", traindir, \"6\")\n",
    "  if batch_size:\n",
    "    del FLAGS.batch_size\n",
    "    flags.DEFINE_integer(\"batch_size\", batch_size, \"7\")\n",
    "  if seq_len:\n",
    "    del FLAGS.seq_len\n",
    "    flags.DEFINE_integer(\"seq_len\", seq_len, \"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1643131396392,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "iEyY84OK1HkQ"
   },
   "outputs": [],
   "source": [
    "adjust_para(hidden_size_g=250, hidden_size_d=250, max_epoch=1050, pretraining_epochs=100, reg_scale=0.6, traindir=\"sine_output\", batch_size=128, seq_len=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1643131381697,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "kefzuCBKnVxJ"
   },
   "outputs": [],
   "source": [
    "if not FLAGS.datadir:\n",
    "  raise ValueError(\"Must set --datadir to midi music dir.\")\n",
    "if not FLAGS.traindir:\n",
    "  raise ValueError(\"Must set --traindir to dir where I can save model and plots.\")\n",
    " \n",
    " \n",
    "summaries_dir = None\n",
    "plots_dir = None\n",
    "generated_data_dir = None\n",
    "\n",
    "summaries_dir = os.path.join(FLAGS.traindir, 'summaries')\n",
    "plots_dir = os.path.join(FLAGS.traindir, 'plots')\n",
    "generated_data_dir = os.path.join(FLAGS.traindir, 'generated_data')\n",
    "try: os.makedirs(FLAGS.traindir)\n",
    "except: pass\n",
    "try: os.makedirs(summaries_dir)\n",
    "except: pass\n",
    "try: os.makedirs(plots_dir)\n",
    "except: pass\n",
    "try: os.makedirs(generated_data_dir)\n",
    "except: pass\n",
    "directorynames = FLAGS.traindir.split('/')\n",
    "experiment_label = ''\n",
    "while not experiment_label:\n",
    "  experiment_label = directorynames.pop()\n",
    "\n",
    "global_step = -1\n",
    "if os.path.exists(os.path.join(FLAGS.traindir, 'global_step.pkl')):\n",
    "  with open(os.path.join(FLAGS.traindir, 'global_step.pkl'), 'rb') as f:\n",
    "    global_step = pkl.load(f)\n",
    "global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1643131381697,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "kefzuCBKnVxJ"
   },
   "outputs": [],
   "source": [
    "global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1643107456921,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "TyzMdoP75o5V",
    "outputId": "f879dd1a-096b-4e2b-ec31-8bd078945902"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(data_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1643131397979,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "MbFTbjseyiEP"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwiJDSRjpe4e"
   },
   "outputs": [],
   "source": [
    "loader = data_utils.DataLoader(FLAGS.datadir, FLAGS.select_validation_percentage, FLAGS.select_test_percentage, data_name = \"sine\")\n",
    "num_features = loader.get_num_features()\n",
    "\n",
    "\n",
    "\n",
    "train_start_time = time.time()\n",
    "checkpoint_path = os.path.join(FLAGS.traindir, \"model.ckpt\")\n",
    "sam_data = []\n",
    "\n",
    "all_data = loader.get_all_oridata()\n",
    "num_sample = len(all_data)\n",
    "#pre_iters = int(num_sample/FLAGS.batch_size)\n",
    "pre_iters = 50\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "with tf.Graph().as_default(), tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=FLAGS.log_device_placement)) as session:\n",
    "  with tf.compat.v1.variable_scope(\"model\", reuse=tf.compat.v1.AUTO_REUSE) as scope:\n",
    "      \n",
    "    scope.set_regularizer(tf.keras.regularizers.l2(FLAGS.reg_scale))\n",
    "    m = RNNGAN(is_training=True, num_features=num_features)\n",
    "\n",
    "  if FLAGS.initialize_d:\n",
    "    vars_to_restore = {}\n",
    "    for v in tf.compat.v1.trainable_variables():\n",
    "      if v.name.startswith('model/G/'):\n",
    "        print(v.name[:-2])\n",
    "        vars_to_restore[v.name[:-2]] = v\n",
    "    saver = tf.compat.v1.train.Saver(vars_to_restore)\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.traindir)\n",
    "    if ckpt and tf.io.gfile.exists(ckpt.model_checkpoint_path):\n",
    "      print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path,end=\" \")\n",
    "      saver.restore(session, ckpt.model_checkpoint_path)\n",
    "      session.run(tf.compat.v1.initialize_variables([v for v in tf.compat.v1.trainable_variables() if v.name.startswith('model/D/')]))\n",
    "    else:\n",
    "      print(\"Created model with fresh parameters.\")\n",
    "      session.run(tf.compat.v1.global_variables_initializer())\n",
    "    saver = tf.compat.v1.train.Saver(tf.compat.v1.all_variables())\n",
    "  else:\n",
    "    saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.traindir)\n",
    "    if ckpt and tf.io.gfile.exists(ckpt.model_checkpoint_path):\n",
    "      print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "      saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "      print(\"Created model with fresh parameters.\")\n",
    "      session.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "  if not FLAGS.sample:\n",
    "    train_g_loss,train_d_loss = 1.0,1.0\n",
    "    for i in range(global_step, FLAGS.max_epoch+pre_iters):\n",
    "      lr_decay = FLAGS.lr_decay ** max(i - FLAGS.epochs_before_decay, 0.0)\n",
    "\n",
    "      if not FLAGS.adam:\n",
    "        m.assign_lr(session, FLAGS.learning_rate * lr_decay)\n",
    "\n",
    "      save = False\n",
    "      do_exit = False\n",
    "\n",
    "      print(\"Epoch: {} Learning rate: {:.3f}, pretraining: {}\".format(i, session.run(m.lr), (i<FLAGS.pretraining_epochs)))\n",
    "      if i<FLAGS.pretraining_epochs:\n",
    "        opt_d = tf.no_op()\n",
    "        if FLAGS.pretraining_d:\n",
    "          opt_d = m.opt_d\n",
    "        train_g_loss,train_d_loss = run_epoch(session, m, loader, 'train', m.opt_pretraining, opt_d, pretraining = True, verbose=False, pretraining_d=FLAGS.pretraining_d)\n",
    "        if FLAGS.pretraining_d:\n",
    "          try:\n",
    "            print(\"Epoch: {} Pretraining loss: G: {:.3f}, D: {:.3f}\".format(i, train_g_loss, train_d_loss))\n",
    "          except:\n",
    "            print(train_g_loss)\n",
    "            print(train_d_loss)\n",
    "        else:\n",
    "          print(\"Epoch: {} Pretraining loss: G: {:.3f}\".format(i, train_g_loss))\n",
    "      else:\n",
    "        if i<FLAGS.max_epoch:\n",
    "          train_g_loss,train_d_loss = run_epoch(session, m, loader, 'train', m.opt_d, m.opt_g, verbose=True, predictive=False)\n",
    "        else:\n",
    "          train_g_loss,train_d_loss = run_epoch(session, m, loader, 'train', m.opt_d, m.opt_g, verbose=True, predictive=True)\n",
    "          sam_data.append(sample(session, m))\n",
    "          print(\"GENERATED DATA:\", len(m.generated_features), m.generated_features[0].get_shape())\n",
    "        try:\n",
    "          print(\"Epoch: {} Train loss: G: {:.3f}, D: {:.3f}\".format(i, train_g_loss, train_d_loss))\n",
    "        except:\n",
    "          print(\"Epoch: {} Train loss: G: {}, D: {}\".format(i, train_g_loss, train_d_loss))\n",
    "      \n",
    "      if FLAGS.select_validation_percentage:\n",
    "        valid_g_loss, valid_d_loss = run_epoch(session, m, loader, 'validation', tf.no_op(), tf.no_op())\n",
    "        try:\n",
    "          print(\"Epoch: {} Valid loss: G: {:.3f}, D: {:.3f}\".format(i, valid_g_loss, valid_d_loss))\n",
    "        except:\n",
    "          print(\"Epoch: {} Valid loss: G: {}, D: {}\".format(i, valid_g_loss, valid_d_loss))\n",
    "      \n",
    "      if train_d_loss == 0.0 and train_g_loss == 0.0:\n",
    "        print('Both G and D train loss are zero. Exiting.')\n",
    "        save = True\n",
    "        do_exit = True\n",
    "      if i % FLAGS.epochs_per_checkpoint == 0:\n",
    "        save = True\n",
    "      if FLAGS.exit_after > 0 and time.time() - train_start_time > FLAGS.exit_after*60:\n",
    "        print(\"%s: Has been running for %d seconds. Will exit (exiting after %d minutes).\"%(datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S'), (int)(time.time() - train_start_time), FLAGS.exit_after))\n",
    "        save = True\n",
    "        do_exit = True\n",
    "\n",
    "      if save:\n",
    "        saver.save(session, checkpoint_path, global_step=i)\n",
    "        with open(os.path.join(FLAGS.traindir, 'global_step.pkl'), 'wb') as f:\n",
    "          pkl.dump(i, f)\n",
    "        print('{}: Saving done!'.format(i))\n",
    "\n",
    "      step_time, loss = 0.0, 0.0\n",
    "      if train_d_loss is None: #pretraining\n",
    "        train_d_loss = 0.0\n",
    "        valid_d_loss = 0.0\n",
    "        valid_g_loss = 0.0\n",
    "      if not os.path.exists(os.path.join(plots_dir, 'gnuplot-input.txt')):\n",
    "        with open(os.path.join(plots_dir, 'gnuplot-input.txt'), 'w') as f:\n",
    "          f.write('# global-step learning-rate train-g-loss train-d-loss valid-g-loss valid-d-loss\\n')\n",
    "      with open(os.path.join(plots_dir, 'gnuplot-input.txt'), 'a') as f:\n",
    "        try:\n",
    "          f.write('{} {:.4f} {:.2f} {:.2f} {:.3} {:.3f}\\n'.format(i, m.lr.eval(), train_g_loss, train_d_loss, valid_g_loss, valid_d_loss))\n",
    "        except:\n",
    "          f.write('{} {} {} {}\\n'.format(i, m.lr.eval(), train_g_loss, train_d_loss))\n",
    "      if not os.path.exists(os.path.join(plots_dir, 'gnuplot-commands-loss.txt')):\n",
    "        with open(os.path.join(plots_dir, 'gnuplot-commands-loss.txt'), 'a') as f:\n",
    "          f.write('set terminal postscript eps color butt \"Times\" 14\\nset yrange [0:400]\\nset output \"loss.eps\"\\nplot \\'gnuplot-input.txt\\' using ($1):($3) title \\'train G\\' with linespoints, \\'gnuplot-input.txt\\' using ($1):($4) title \\'train D\\' with linespoints, \\'gnuplot-input.txt\\' using ($1):($5) title \\'valid G\\' with linespoints, \\'gnuplot-input.txt\\' using ($1):($6) title \\'valid D\\' with linespoints, \\n')\n",
    "      try:\n",
    "        Popen(['gnuplot','gnuplot-commands-loss.txt'], cwd=plots_dir)\n",
    "      except:\n",
    "        print('failed to run gnuplot. Please do so yourself: gnuplot gnuplot-commands.txt cwd={}'.format(plots_dir))\n",
    "      \n",
    "      if FLAGS.select_test_percentage:\n",
    "        test_g_loss,test_d_loss = run_epoch(session, m, loader, 'test', tf.no_op(), tf.no_op())\n",
    "        print(\"Test loss G: %.3f, D: %.3f\" %(test_g_loss, test_d_loss))\n",
    "      \n",
    "\n",
    "        \n",
    "      \n",
    "\n",
    "  \n",
    "  sam_data.append(sample(session, m))\n",
    "  #filename = os.path.join(generated_data_dir, 'out-{}-{}-{}.csv'.format(experiment_label, i, datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S')))\n",
    "  #with open(filename, 'w') as f:\n",
    "  #  writer = csv.writer(f)\n",
    "  #  writer.writerows(sam_data)\n",
    "  #  print('Saved {}.'.format(filename)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1643115378943,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "c-dvcOw3y2SB"
   },
   "outputs": [],
   "source": [
    "gen_data = np.array(sam_data).reshape(-1, 24, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1643115441327,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "Scz7chRWkkp9"
   },
   "outputs": [],
   "source": [
    "import visualization_metrics\n",
    "import predictive_metrics\n",
    "import discriminative_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1643115389189,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "CWOKSLurkt_d"
   },
   "outputs": [],
   "source": [
    "#loader = data_utils.DataLoader(FLAGS.datadir, FLAGS.select_validation_percentage, FLAGS.select_test_percentage, data_name = \"stock_data\")\n",
    "ori_data = np.array(loader.get_all_oridata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1643115387273,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "4BaWwGMfLXND",
    "outputId": "e9468544-9e38-4ec2-e35c-5f3f97552e50"
   },
   "outputs": [],
   "source": [
    "ori_filename = os.path.join(generated_data_dir, 'ori_data.npy')\n",
    "gen_filename = os.path.join(generated_data_dir, 'gen_data.npy')\n",
    "np.save(ori_filename, ori_data)\n",
    "np.save(gen_filename, gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1643115387273,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "4BaWwGMfLXND",
    "outputId": "e9468544-9e38-4ec2-e35c-5f3f97552e50"
   },
   "outputs": [],
   "source": [
    "ori_filename = os.path.join(generated_data_dir, 'ori_data.npy')\n",
    "gen_filename = os.path.join(generated_data_dir, 'gen_data.npy')\n",
    "ori_data = np.load(ori_filename)\n",
    "gen_data = np.load(gen_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1643115390735,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "yRkCoIIhyf_r",
    "outputId": "5e6f22e5-1c1c-4c63-fd5d-398370310c55"
   },
   "outputs": [],
   "source": [
    "gen_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1643115391792,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "VK51UN5VHuxG",
    "outputId": "6d2f86a1-586c-4eb2-b491-1abf8467254e"
   },
   "outputs": [],
   "source": [
    "ori_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1643115391792,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "VK51UN5VHuxG",
    "outputId": "6d2f86a1-586c-4eb2-b491-1abf8467254e"
   },
   "outputs": [],
   "source": [
    "minn = loader.get_minn()\n",
    "deno = loader.get_deno()\n",
    "ori_data_oriscale = ori_data*deno + minn\n",
    "gen_data_oriscale = gen_data*deno + minn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "executionInfo": {
     "elapsed": 13781,
     "status": "ok",
     "timestamp": 1643115477610,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "tmvkLD3yAcgq",
    "outputId": "689f64c9-d79a-46ba-acb4-155c33a7f8c1"
   },
   "outputs": [],
   "source": [
    "visualization_metrics.visualization(ori_data_oriscale[1600:2600], gen_data_oriscale[1600:2600], 'pca')\n",
    "visualization_metrics.visualization(ori_data_oriscale[1600:2600], gen_data_oriscale[1600:2600], 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "executionInfo": {
     "elapsed": 15165,
     "status": "ok",
     "timestamp": 1643113306360,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "6mZbdaxGLtg4",
    "outputId": "5d9001ad-8ca5-43c8-a438-b2849efd5f50"
   },
   "outputs": [],
   "source": [
    "visualization_metrics.visualization(ori_data[0:1000], gen_data[0:1000], 'pca')\n",
    "visualization_metrics.visualization(ori_data[0:1000], gen_data[0:1000], 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1084450,
     "status": "ok",
     "timestamp": 1643116562048,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "hR1P_ILBH3Gn",
    "outputId": "9419e3a9-5a53-4963-da34-38126d6efb61"
   },
   "outputs": [],
   "source": [
    "predictive_metrics.predictive_score_metrics(ori_data[0:6528], gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1084450,
     "status": "ok",
     "timestamp": 1643116562048,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "hR1P_ILBH3Gn",
    "outputId": "9419e3a9-5a53-4963-da34-38126d6efb61"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "p = []\n",
    "d = []\n",
    "for i in range(100,200):\n",
    "  print(i)\n",
    "  p.append(predictive_metrics.predictive_score_metrics(ori_data[(i+1300):(i+2300)], gen_data[(i+1300):(i+2300)]))\n",
    "  d.append(discriminative_metrics.discriminative_score_metrics(ori_data[(i+1300):(i+2300)], gen_data[(i+1300):(i+2300)]))\n",
    "  if i%10==1:\n",
    "        filename = os.path.join(generated_data_dir, 'pd_score.txt')\n",
    "        pd = np.vstack([np.array(p), np.array(d)])\n",
    "        with open(filename, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1643117720729,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "Dh_d3ePozNES",
    "outputId": "0046dec9-a065-4382-c737-732d43477ff8"
   },
   "outputs": [],
   "source": [
    " filename = os.path.join(generated_data_dir, 'score.txt')\n",
    " with open(filename, 'w') as f:\n",
    "   writer = csv.writer(f)\n",
    "   writer.writerow([np.mean(p), np.mean(d), np.std(p), np.std(d)])\n",
    "   print('Saved {}.'.format(filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1643117720729,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "Dh_d3ePozNES",
    "outputId": "0046dec9-a065-4382-c737-732d43477ff8"
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1643113065778,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "jru0TUyuK6Xp",
    "outputId": "54eac5af-3c2c-401a-8719-1f739adbc560"
   },
   "outputs": [],
   "source": [
    "print(np.mean(p), np.mean(d))\n",
    "print(np.std(p), np.std(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1643096708611,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "mZqcriSkIUWg",
    "outputId": "28283b1f-3f0d-4e3c-f405-00f14fb7bdb1"
   },
   "outputs": [],
   "source": [
    "importlib.reload(predictive_metrics)\n",
    "importlib.reload(discriminative_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5682,
     "status": "ok",
     "timestamp": 1642603836164,
     "user": {
      "displayName": "Liu Mengqi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07034097190770324116"
     },
     "user_tz": -480
    },
    "id": "V-7rSzQWxdVw",
    "outputId": "0624a8be-1462-4d7b-db33-85d7e34d06dd"
   },
   "outputs": [],
   "source": [
    " filename = os.path.join(generated_data_dir, 'out-{}-{}.txt'.format(experiment_label, datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S')))\n",
    " with open(filename, 'w') as f:\n",
    "   writer = csv.writer(f)\n",
    "   writer.writerows(sam_data2)\n",
    "   print('Saved {}.'.format(filename)) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNfXk39SAjLBVikEk2NhVWT",
   "collapsed_sections": [],
   "mount_file_id": "1jKgLLm3cuA1ZCt5VMWnS-mAnHOjSyJqa",
   "name": "C-RNN-GAN-for TIMEGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "time-env",
   "language": "python",
   "name": "time_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
